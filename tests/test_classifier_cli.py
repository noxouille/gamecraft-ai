#!/usr/bin/env python3
"""CLI test script for ClassifierAgent with real API calls"""

import argparse
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.gamecraft_ai.agents.classifier import ClassifierAgent, RelevanceError  # noqa: E402
from src.gamecraft_ai.models import QueryType  # noqa: E402
from src.gamecraft_ai.models.query import QueryInput  # noqa: E402


def test_classifier(query_text: str, duration: int = 10, model: str = "gpt-4o-mini") -> None:
    """Test the classifier agent with a given query"""
    print(f"ğŸ” Testing ClassifierAgent with model: {model}")
    print(f"ğŸ“ Query: '{query_text}'")
    print(f"â±ï¸  Duration: {duration} minutes")
    print("=" * 70)

    try:
        # Initialize classifier agent
        classifier = ClassifierAgent(model=model)

        # Create query input
        query = QueryInput(text=query_text, duration_minutes=duration)

        # Create state dict as expected by the agent
        state = {"query": query}

        print("ğŸ¤– Calling classifier agent...")

        # Classify the query
        result_state = classifier.classify_query(state)

        # Extract results
        classified_query = result_state["query"]
        metadata = result_state["query_metadata"]

        print("âœ… Classification successful!")
        print("-" * 50)

        # Display results
        print(f"ğŸ¯ Query Type: {classified_query.query_type.value}")
        print(f"ğŸŒ Language: {classified_query.language.value}")
        print(f"ğŸ“Š Confidence: {metadata['confidence']:.2f}")

        if metadata.get("game_name"):
            print(f"ğŸ® Game Name: {metadata['game_name']}")

        if metadata.get("video_url"):
            print(f"ğŸ”— Video URL: {metadata['video_url']}")

        print(f"ğŸ“‹ Script Format: {metadata['script_format']}")

        # Show the flow based on query type
        print("\n" + "=" * 50)
        print("ğŸ“ˆ COMPLETE WORKFLOW PATH:")
        if classified_query.query_type == QueryType.EVENT:
            print("ğŸª Event Analysis Flow:")
            print("   1. âœ… Classifier Agent â†’ Query validated and classified")
            print("   2. ğŸ” Research Agent â†’ Analyze event video & gather info")
            print("   3. âœï¸  Script Writer â†’ Create event summary script")
            print("   4. ğŸ–¼ï¸  YouTube Coach â†’ Generate viral thumbnail prompts")
        else:
            print("ğŸ® Game Content Flow:")
            print("   1. âœ… Classifier Agent â†’ Query validated and classified")
            print("   2. ğŸ” Research Agent â†’ Gather game information & media")
            print("   3. âœï¸  Script Writer â†’ Create game review/preview script")
            print("   4. ğŸ–¼ï¸  YouTube Coach â†’ Generate viral thumbnail prompts")

        print("\nğŸ‰ Ready to proceed with complete 4-agent workflow!")

    except RelevanceError as e:
        print("âŒ RELEVANCE CHECK FAILED")
        print("-" * 50)
        print(f"ğŸš« {e}")
        print("\nğŸ’¡ Please provide a query related to:")
        print("   â€¢ Video game content (reviews, previews, gameplay)")
        print("   â€¢ Gaming events (showcases, conferences, streams)")
        print("   â€¢ YouTube gaming content creation")
        print("   â€¢ Gaming industry topics")

    except Exception as e:
        print("âŒ ERROR OCCURRED")
        print("-" * 50)
        print(f"ğŸ”¥ {type(e).__name__}: {e}")


def run_interactive_mode(model: str = "gpt-4o-mini") -> None:
    """Run interactive testing mode"""
    print("ğŸ® Interactive ClassifierAgent Tester")
    print("=" * 50)
    print("Enter queries to test relevance and classification.")
    print("Type 'quit' or 'exit' to stop.")
    print()

    while True:
        try:
            query = input("ğŸ“ Enter query: ").strip()

            if query.lower() in ["quit", "exit", "q"]:
                print("ğŸ‘‹ Goodbye!")
                break

            if not query:
                print("âš ï¸  Please enter a query.")
                continue

            # Ask for duration
            duration_input = input("â±ï¸  Duration (minutes, default=10): ").strip()
            try:
                duration = int(duration_input) if duration_input else 10
            except ValueError:
                duration = 10
                print("âš ï¸  Invalid duration, using default (10 minutes)")

            print()
            test_classifier(query, duration, model)
            print("\n" + "=" * 70 + "\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Unexpected error: {e}")


def run_batch_tests(model: str = "gpt-4o-mini") -> None:
    """Run predefined batch tests"""
    test_cases = [
        # Relevant gaming queries
        ("Create a 10-minute review script for Zelda: Breath of the Wild", 10),
        ("Summarize the Nintendo Direct showcase from yesterday", 15),
        ("Write a preview video about Cyberpunk 2077", 8),
        ("Make thumbnail ideas for my gaming channel", 5),
        ("Analyze this gaming event: https://youtube.com/watch?v=abc123", 20),
        ("CrÃ©e une critique de 12 minutes sur Mario Odyssey", 12),
        # Non-relevant queries (should be rejected)
        ("How to cook spaghetti carbonara", 10),
        ("What's the weather like today?", 5),
        ("Write a business plan for my startup", 15),
        ("Explain quantum physics concepts", 10),
        ("How to learn Python programming", 8),
    ]

    print("ğŸ”¬ Running Batch Tests")
    print("=" * 70)

    passed = 0
    failed = 0

    for i, (query, duration) in enumerate(test_cases, 1):
        print(f"\nğŸ§ª Test Case {i}/{len(test_cases)}")
        print("-" * 30)

        try:
            test_classifier(query, duration, model)
            if i <= 6:  # First 6 should pass
                passed += 1
                print("âœ… Expected: PASS - Result: PASS")
            else:  # Last 5 should fail
                failed += 1
                print("âŒ Expected: FAIL - Result: PASS (False Positive)")
        except RelevanceError:
            if i > 6:  # Last 5 should fail
                passed += 1
                print("âœ… Expected: FAIL - Result: FAIL")
            else:  # First 6 should pass
                failed += 1
                print("âŒ Expected: PASS - Result: FAIL (False Negative)")
        except Exception as e:
            failed += 1
            print(f"âŒ Test failed with error: {e}")

        print("=" * 50)

    print("\nğŸ“Š BATCH TEST RESULTS:")
    print(f"âœ… Passed: {passed}/{len(test_cases)}")
    print(f"âŒ Failed: {failed}/{len(test_cases)}")
    print(f"ğŸ“ˆ Success Rate: {(passed/len(test_cases)*100):.1f}%")


def main():
    parser = argparse.ArgumentParser(description="Test ClassifierAgent with real API calls")
    parser.add_argument("--query", help="Single query to test")
    parser.add_argument(
        "--duration", type=int, default=10, help="Duration in minutes (default: 10)"
    )
    parser.add_argument(
        "--model", default="gpt-4o-mini", help="LLM model to use (default: gpt-4o-mini)"
    )
    parser.add_argument("--interactive", action="store_true", help="Run in interactive mode")
    parser.add_argument("--batch", action="store_true", help="Run predefined batch tests")

    args = parser.parse_args()

    if args.batch:
        run_batch_tests(args.model)
    elif args.interactive:
        run_interactive_mode(args.model)
    elif args.query:
        test_classifier(args.query, args.duration, args.model)
    else:
        # Default to interactive mode
        print("No specific test specified, running interactive mode...")
        print("Use --help to see all options.")
        print()
        run_interactive_mode(args.model)


if __name__ == "__main__":
    main()
